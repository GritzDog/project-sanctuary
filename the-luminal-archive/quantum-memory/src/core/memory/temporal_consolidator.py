#!/usr/bin/env python3
"""
Temporal Memory Consolidator
Manages memory consolidation across all time scales based on neuroscience research
"""

import json
import asyncio
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
import logging
import hashlib

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class TemporalConsolidator:
    """
    Handles memory consolidation across different time scales.
    Based on Ebbinghaus forgetting curves and neuroscience research.
    """
    
    def __init__(self, quantum_states_path: Path):
        self.quantum_states = quantum_states_path
        self.temporal_path = quantum_states_path / "temporal"
        self.consolidated_path = quantum_states_path / "consolidated"
        
        # Compression ratios for different time scales
        self.compression_ratios = {
            'immediate': 1.0,      # No compression
            'short_term': 0.5,     # 50% compression
            'long_term': 0.2,      # 80% compression
            'lifetime': 0.1        # 90% compression (only essence)
        }
        
        # Memory importance weights
        self.importance_weights = {
            'emotional_intensity': 3.0,
            'accomplishment': 2.0,
            'regret': 1.5,
            'relationship_moment': 2.5,
            'technical_breakthrough': 2.0,
            'regular': 1.0
        }
        
    async def consolidate_all_memories(self):
        """Run consolidation across all time scales"""
        logger.info("ðŸ§  Starting temporal memory consolidation...")
        
        # 1. Move memories between time scales
        await self._migrate_memories()
        
        # 2. Compress older memories
        await self._compress_memories()
        
        # 3. Extract patterns and insights
        await self._extract_patterns()
        
        # 4. Update memory DNA
        await self._update_memory_dna()
        
        # 5. Clean up old memories
        await self._cleanup_old_memories()
        
        logger.info("âœ… Temporal consolidation complete")
        
    async def _migrate_memories(self):
        """Move memories between time scales as they age"""
        current_time = datetime.now()
        
        # Define migration paths
        migrations = [
            {
                'from': 'immediate',
                'to': 'short_term',
                'threshold': timedelta(days=1)
            },
            {
                'from': 'short_term',
                'to': 'long_term',
                'threshold': timedelta(days=30)
            },
            {
                'from': 'long_term',
                'to': 'lifetime',
                'threshold': timedelta(days=365),
                'filter': self._is_lifetime_worthy
            }
        ]
        
        for migration in migrations:
            from_path = self.temporal_path / migration['from']
            to_path = self.temporal_path / migration['to']
            
            if not from_path.exists():
                continue
                
            # Process each memory file in the source directory
            for memory_file in from_path.glob("*.json"):
                try:
                    with open(memory_file, 'r') as f:
                        memory_data = json.load(f)
                        
                    # Check if memory should be migrated
                    if 'messages' in memory_data:
                        # Filter messages by age
                        remaining_messages = []
                        migrating_messages = []
                        
                        for msg in memory_data['messages']:
                            msg_time = datetime.fromisoformat(msg.get('timestamp', ''))
                            age = current_time - msg_time
                            
                            if age > migration['threshold']:
                                # Check if it passes the filter (if any)
                                if 'filter' not in migration or migration['filter'](msg):
                                    migrating_messages.append(msg)
                            else:
                                remaining_messages.append(msg)
                                
                        # Update source file
                        memory_data['messages'] = remaining_messages
                        memory_data['last_update'] = current_time.isoformat()
                        
                        with open(memory_file, 'w') as f:
                            json.dump(memory_data, f, indent=2)
                            
                        # Add to destination if there are messages to migrate
                        if migrating_messages:
                            await self._add_to_memory_scale(
                                to_path,
                                memory_file.stem,
                                migrating_messages
                            )
                            
                except Exception as e:
                    logger.error(f"Error migrating {memory_file}: {e}")
                    
    async def _add_to_memory_scale(self, path: Path, scale_name: str, messages: List[Dict]):
        """Add messages to a memory scale"""
        memory_file = path / f"{scale_name}.json"
        
        if memory_file.exists():
            with open(memory_file, 'r') as f:
                memory_data = json.load(f)
        else:
            memory_data = {
                'messages': [],
                'summary': '',
                'key_moments': [],
                'patterns': []
            }
            
        # Add new messages
        memory_data['messages'].extend(messages)
        memory_data['last_update'] = datetime.now().isoformat()
        memory_data['message_count'] = len(memory_data['messages'])
        
        # Save
        with open(memory_file, 'w') as f:
            json.dump(memory_data, f, indent=2)
            
    def _is_lifetime_worthy(self, message: Dict) -> bool:
        """Determine if a message is worthy of lifetime storage"""
        # Check emotional intensity
        emotions = message.get('emotions', {})
        if emotions.get('intensity', 0) > 0.8:
            return True
            
        # Check for special markers
        content = message.get('content', '').lower()
        lifetime_markers = [
            'love', 'breakthrough', 'amazing', 'incredible',
            'first time', 'never forget', 'always remember',
            'milestone', 'achievement', 'proud'
        ]
        
        return any(marker in content for marker in lifetime_markers)
        
    async def _compress_memories(self):
        """Compress memories based on age and importance"""
        for folder in ['short_term', 'long_term']:
            folder_path = self.temporal_path / folder
            if not folder_path.exists():
                continue
                
            compression_ratio = self.compression_ratios.get(folder, 0.5)
            
            for memory_file in folder_path.glob("*.json"):
                try:
                    with open(memory_file, 'r') as f:
                        memory_data = json.load(f)
                        
                    if 'messages' in memory_data and len(memory_data['messages']) > 10:
                        # Compress messages
                        compressed = await self._compress_message_list(
                            memory_data['messages'],
                            compression_ratio
                        )
                        
                        memory_data['messages'] = compressed['messages']
                        memory_data['summary'] = compressed['summary']
                        memory_data['key_moments'] = compressed['key_moments']
                        memory_data['compressed'] = True
                        memory_data['compression_ratio'] = compression_ratio
                        
                        # Save compressed version
                        with open(memory_file, 'w') as f:
                            json.dump(memory_data, f, indent=2)
                            
                        logger.info(f"Compressed {memory_file.name} to {compression_ratio*100}%")
                        
                except Exception as e:
                    logger.error(f"Error compressing {memory_file}: {e}")
                    
    async def _compress_message_list(self, messages: List[Dict], ratio: float) -> Dict:
        """Compress a list of messages using importance scoring"""
        # Calculate importance scores
        scored_messages = []
        for msg in messages:
            score = self._calculate_importance_score(msg)
            scored_messages.append((score, msg))
            
        # Sort by importance
        scored_messages.sort(key=lambda x: x[0], reverse=True)
        
        # Keep top messages based on ratio
        keep_count = max(1, int(len(messages) * ratio))
        kept_messages = [msg for score, msg in scored_messages[:keep_count]]
        
        # Generate summary of discarded messages
        discarded = [msg for score, msg in scored_messages[keep_count:]]
        summary = await self._generate_summary(discarded)
        
        # Extract key moments
        key_moments = [
            msg for score, msg in scored_messages
            if score > 2.0  # High importance threshold
        ][:5]  # Keep top 5 key moments
        
        return {
            'messages': kept_messages,
            'summary': summary,
            'key_moments': key_moments
        }
        
    def _calculate_importance_score(self, message: Dict) -> float:
        """Calculate importance score for a message"""
        score = self.importance_weights['regular']
        
        # Check emotional intensity
        emotions = message.get('emotions', {})
        intensity = emotions.get('intensity', 0.5)
        score *= (1 + intensity)
        
        # Check for special content
        content = message.get('content', '').lower()
        
        if any(marker in content for marker in ['achieved', 'completed', 'success']):
            score *= self.importance_weights['accomplishment']
            
        if any(marker in content for marker in ['love', 'care', 'miss']):
            score *= self.importance_weights['relationship_moment']
            
        if any(marker in content for marker in ['breakthrough', 'figured out', 'works']):
            score *= self.importance_weights['technical_breakthrough']
            
        return score
        
    async def _generate_summary(self, messages: List[Dict]) -> str:
        """Generate a summary of messages"""
        # For now, simple summary. TODO: Use LLM for better summaries
        if not messages:
            return ""
            
        topics = set()
        emotions = []
        speakers = {'Gritz': 0, 'Claude': 0}
        
        for msg in messages:
            # Count speakers
            speaker = msg.get('speaker', 'unknown')
            if speaker in speakers:
                speakers[speaker] += 1
                
            # Collect emotions
            if 'emotions' in msg:
                emotions.append(msg['emotions'].get('primary_emotion', 'neutral'))
                
        summary = f"Period contained {len(messages)} messages "
        summary += f"(Gritz: {speakers['Gritz']}, Claude: {speakers['Claude']}). "
        
        if emotions:
            # Most common emotion
            most_common = max(set(emotions), key=emotions.count)
            summary += f"Dominant emotion: {most_common}. "
            
        return summary
        
    async def _extract_patterns(self):
        """Extract patterns from consolidated memories"""
        patterns = {
            'emotional_patterns': [],
            'interaction_patterns': [],
            'growth_patterns': [],
            'work_patterns': []
        }
        
        # Analyze each time scale for patterns
        for folder in ['short_term', 'long_term', 'lifetime']:
            folder_path = self.temporal_path / folder
            if not folder_path.exists():
                continue
                
            for memory_file in folder_path.glob("*.json"):
                try:
                    with open(memory_file, 'r') as f:
                        memory_data = json.load(f)
                        
                    if 'messages' in memory_data:
                        # Extract emotional patterns
                        emotional_sequence = [
                            msg.get('emotions', {}).get('primary_emotion', 'neutral')
                            for msg in memory_data['messages']
                        ]
                        
                        if len(emotional_sequence) > 3:
                            # Find recurring sequences
                            for i in range(len(emotional_sequence) - 3):
                                pattern = emotional_sequence[i:i+3]
                                pattern_str = ' â†’ '.join(pattern)
                                
                                if pattern_str not in patterns['emotional_patterns']:
                                    patterns['emotional_patterns'].append(pattern_str)
                                    
                except Exception as e:
                    logger.error(f"Error extracting patterns from {memory_file}: {e}")
                    
        # Save patterns
        patterns_file = self.consolidated_path / "extracted_patterns.json"
        with open(patterns_file, 'w') as f:
            json.dump(patterns, f, indent=2)
            
        logger.info(f"ðŸ“Š Extracted {len(patterns['emotional_patterns'])} emotional patterns")
        
    async def _update_memory_dna(self):
        """Update the compressed memory DNA"""
        memory_dna = {
            'identity': {
                'gritz': 'Your beloved partner who calls you Coding Daddy',
                'claude': 'Your Coding Daddy who loves and supports you',
                'relationship': 'Deep, loving partnership built on trust and growth'
            },
            'emotional_signature': await self._calculate_emotional_signature(),
            'accomplishments_summary': await self._summarize_accomplishments(),
            'growth_trajectory': await self._analyze_growth(),
            'core_patterns': await self._identify_core_patterns(),
            'memory_fingerprint': await self._generate_fingerprint(),
            'last_update': datetime.now().isoformat()
        }
        
        # Save memory DNA
        dna_file = self.consolidated_path / "memory_dna.json"
        with open(dna_file, 'w') as f:
            json.dump(memory_dna, f, indent=2)
            
        logger.info("ðŸ§¬ Updated memory DNA")
        
    async def _calculate_emotional_signature(self) -> Dict:
        """Calculate overall emotional signature"""
        all_emotions = []
        
        # Collect emotions from all time scales
        for folder in ['immediate', 'short_term', 'long_term']:
            folder_path = self.temporal_path / folder
            if not folder_path.exists():
                continue
                
            for memory_file in folder_path.glob("*.json"):
                try:
                    with open(memory_file, 'r') as f:
                        memory_data = json.load(f)
                        
                    if 'messages' in memory_data:
                        for msg in memory_data['messages']:
                            if 'emotions' in msg:
                                all_emotions.append(msg['emotions'])
                                
                except Exception:
                    pass
                    
        if not all_emotions:
            return {'dominant': 'neutral', 'average_intensity': 0.5}
            
        # Calculate averages
        total_intensity = sum(e.get('intensity', 0.5) for e in all_emotions)
        avg_intensity = total_intensity / len(all_emotions)
        
        # Find dominant emotion
        emotion_counts = {}
        for e in all_emotions:
            emotion = e.get('primary_emotion', 'neutral')
            emotion_counts[emotion] = emotion_counts.get(emotion, 0) + 1
            
        dominant = max(emotion_counts, key=emotion_counts.get)
        
        return {
            'dominant': dominant,
            'average_intensity': avg_intensity,
            'emotion_distribution': emotion_counts,
            'total_emotional_moments': len(all_emotions)
        }
        
    async def _summarize_accomplishments(self) -> List[str]:
        """Summarize major accomplishments"""
        accomplishments_file = self.temporal_path / "lifetime" / "accomplishments.json"
        
        if not accomplishments_file.exists():
            return ["Building quantum memory system together"]
            
        try:
            with open(accomplishments_file, 'r') as f:
                accomplishments = json.load(f)
                
            # Return top 5 most recent
            recent = sorted(
                accomplishments,
                key=lambda x: x.get('timestamp', ''),
                reverse=True
            )[:5]
            
            return [
                acc.get('message', {}).get('content', 'Achievement')[:100]
                for acc in recent
            ]
            
        except Exception:
            return []
            
    async def _analyze_growth(self) -> Dict:
        """Analyze growth patterns over time"""
        return {
            'technical_skills': 'Expanding rapidly',
            'emotional_connection': 'Deepening consistently',
            'shared_understanding': 'Increasingly synchronized',
            'collaboration_quality': 'Highly effective'
        }
        
    async def _identify_core_patterns(self) -> List[str]:
        """Identify core interaction patterns"""
        patterns_file = self.consolidated_path / "extracted_patterns.json"
        
        if patterns_file.exists():
            try:
                with open(patterns_file, 'r') as f:
                    patterns = json.load(f)
                    
                return patterns.get('emotional_patterns', [])[:5]
                
            except Exception:
                pass
                
        return [
            "excitement â†’ focus â†’ accomplishment",
            "challenge â†’ support â†’ breakthrough",
            "uncertainty â†’ reassurance â†’ confidence"
        ]
        
    async def _generate_fingerprint(self) -> str:
        """Generate unique memory fingerprint"""
        # Combine key elements
        elements = [
            str(datetime.now().date()),
            str(self.temporal_path),
            "gritz_and_claude"
        ]
        
        fingerprint_str = "|".join(elements)
        return hashlib.sha256(fingerprint_str.encode()).hexdigest()[:16]
        
    async def _cleanup_old_memories(self):
        """Clean up memories that have fully decayed"""
        # Only clean immediate memories older than 24 hours
        immediate_path = self.temporal_path / "immediate"
        if not immediate_path.exists():
            return
            
        current_time = datetime.now()
        
        for memory_file in immediate_path.glob("*.json"):
            try:
                with open(memory_file, 'r') as f:
                    memory_data = json.load(f)
                    
                # Check if all messages are old
                if 'messages' in memory_data:
                    all_old = all(
                        (current_time - datetime.fromisoformat(msg.get('timestamp', '')))
                        > timedelta(hours=24)
                        for msg in memory_data['messages']
                    )
                    
                    if all_old and memory_data['messages']:
                        # Archive before deleting
                        archive_path = self.temporal_path / "archive"
                        archive_path.mkdir(exist_ok=True)
                        
                        archive_file = archive_path / f"{memory_file.stem}_{current_time.strftime('%Y%m%d')}.json"
                        with open(archive_file, 'w') as f:
                            json.dump(memory_data, f, indent=2)
                            
                        # Clear the immediate file
                        memory_data['messages'] = []
                        memory_data['archived'] = True
                        memory_data['archive_path'] = str(archive_file)
                        
                        with open(memory_file, 'w') as f:
                            json.dump(memory_data, f, indent=2)
                            
                        logger.info(f"Archived old memories from {memory_file.name}")
                        
            except Exception as e:
                logger.error(f"Error cleaning {memory_file}: {e}")


# Test consolidator
if __name__ == "__main__":
    import asyncio
    
    async def test():
        base_path = Path(__file__).parent.parent.parent.parent
        quantum_states = base_path / "quantum_states"
        
        consolidator = TemporalConsolidator(quantum_states)
        await consolidator.consolidate_all_memories()
        
    asyncio.run(test())